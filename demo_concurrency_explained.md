# Understanding Concurrency Control in LangGraph ğŸ§ 

## The Two-Layer System

When using LangGraph for parallel processing, you actually have **TWO layers** of concurrency control:

### Layer 1: LangGraph's Parallel Execution (Framework Level)
**What it does:**
- Uses `Send()` API to spawn parallel branches
- Automatically waits for all branches to complete
- Merges state using reducers (`Annotated[list, add]`)

**What it DOESN'T do:**
- âŒ Limit how many branches run concurrently
- âŒ Queue tasks when resources are constrained
- âŒ Control resource usage

### Layer 2: Your Semaphore (Application Level)
**What it does:**
- âœ… Limits concurrent executions (max N at a time)
- âœ… Queues additional tasks automatically
- âœ… Controls resource usage (worktrees, API calls, etc.)

**Example:**
```python
# LangGraph launches 100 branches "in parallel"
return [Send("process_file", {...}) for file in 100_files]

# But your semaphore ensures only 4 actually run at once
async with self._semaphore:  # Only 4 slots available
    await process_file()     # Others wait in queue
```

---

## ğŸ¯ In Practice

### Without Semaphore (BAD)
```python
# LangGraph launches ALL 100 file processing tasks immediately!
# Result: 100 worktrees, 100 AI agents, system crash! ğŸ’¥
```

### With Semaphore (GOOD)
```python
# LangGraph launches all 100, but semaphore limits to 4 active
# Result: Only 4 worktrees at once, others queue nicely! âœ…
```

---

## ğŸ“Š The Complete Picture

```
User Requests: "Process 100 files in parallel"
                    â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   LangGraph Layer (Framework)          â”‚
    â”‚   - Creates 100 Send() objects         â”‚
    â”‚   - Schedules all for execution        â”‚
    â”‚   - Manages state merging              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Semaphore Layer (Your Control)       â”‚
    â”‚   - Allows only 4 active at once      â”‚
    â”‚   - Queues the other 96               â”‚
    â”‚   - Releases slots as tasks complete  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Critical Section Layer (Your Lock)   â”‚
    â”‚   - Diff application: 1 at a time     â”‚
    â”‚   - Prevents git conflicts            â”‚
    â”‚   - Serializes commits                â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¤” Why Both?

**Q:** Why not just use semaphore without LangGraph's Send()?

**A:** Because then you lose:
- âŒ Automatic state aggregation (you'd build it manually)
- âŒ Observability (LangSmith integration)
- âŒ Declarative graph structure
- âŒ Framework-managed async lifecycle

**Q:** Why not just use LangGraph's config for concurrency?

**A:** LangGraph doesn't have a `max_concurrency` config that works this way!
- The framework schedules tasks, but doesn't limit concurrent executions
- You need semaphore for resource control

---

## ğŸ“ Rule of Thumb

**Use LangGraph for:**
- ğŸ“Š State management and merging
- ğŸ”€ Parallel execution framework
- ğŸ“ˆ Observability and tracing
- ğŸ—ï¸ Graph structure

**Use Semaphore for:**
- ğŸšï¸ Concurrency limiting (max N concurrent)
- ğŸ’¾ Resource management (memory, API limits)
- âš¡ Performance tuning

**Use Lock for:**
- ğŸ”’ Critical sections (must be serial)
- ğŸ›¡ï¸ Preventing race conditions
- ğŸ“ Atomic operations (like git commits)

---

## âœ… For Stomper

```python
class StomperState(TypedDict, total=False):
    # Aggregated using 'add' reducer
    successful_fixes: Annotated[list[str], add]
    failed_fixes: Annotated[list[str], add]
    
    # Custom reducer for summing
    total_errors_fixed: Annotated[int, lambda x, y: x + y]

class StomperWorkflow:
    def __init__(self, max_parallel_files: int = 4):
        # Layer 1: Concurrency control (limit to N files)
        self._parallel_semaphore = asyncio.Semaphore(max_parallel_files)
        
        # Layer 2: Critical section (serialize diff application)
        self._diff_application_lock = asyncio.Lock()
```

**Perfect architecture!** ğŸŒŸ

