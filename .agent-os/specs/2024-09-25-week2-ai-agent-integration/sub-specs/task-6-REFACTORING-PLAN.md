# Task 6 Refactoring Plan: Per-File Worktree Architecture

> **Status:** ğŸ“‹ READY FOR IMPLEMENTATION  
> **Created:** 2025-10-08  
> **Purpose:** Refactor workflow to use one worktree per file (not per session)  
> **Estimated Time:** 6-8 hours

---

## ğŸ“Š Complete Workflow Visualization

### Detailed Mermaid Diagram - Complete Flow

```mermaid
graph TD
    START([ğŸš€ START]) --> INIT[Initialize Session]
    
    INIT[ğŸ“‹ Initialize Session<br/>Create session ID<br/>Setup session state<br/>NO worktree creation] --> COLLECT
    
    COLLECT[ğŸ” Collect All Errors<br/>Location: MAIN workspace<br/>Run: ruff, mypy, etc<br/>Group by file<br/>Build file queue] --> CHECK_FILES{Any files<br/>with errors?}
    
    CHECK_FILES -->|No errors found| FINAL_CLEANUP
    CHECK_FILES -->|Files to process| CREATE_WT
    
    subgraph PER_FILE_CYCLE ["Per-File Processing Cycle (Repeats for Each File)"]
        CREATE_WT[ğŸ—ï¸ Create Worktree<br/>Location: .stomper/sandboxes/<br/>Name: session_filestem<br/>Branch: sbx/session_filestem<br/>Base: HEAD<br/>FRESH checkout] --> GEN_PROMPT
        
        GEN_PROMPT[ğŸ“ Generate Prompt<br/>Extract THIS file errors only<br/>Load error advice<br/>Apply adaptive strategy<br/>Read code from worktree<br/>Store in current_prompt] --> CALL_AGENT
        
        CALL_AGENT[ğŸ¤– Call AI Agent<br/>Location: IN worktree<br/>Scope: THIS file only<br/>Uses current_prompt<br/>Intelligent fallback<br/>Writes fix to worktree] --> VERIFY
        
        VERIFY[ğŸ” Verify Fixes<br/>Location: IN worktree<br/>Re-run quality tools<br/>Compare before/after<br/>Track fixed errors] --> FIXED{All errors<br/>fixed?}
        
        FIXED -->|Some remain<br/>attempts < max| RETRY
        FIXED -->|Max attempts| ERROR
        FIXED -->|All fixed| TEST_CHECK{Run tests<br/>enabled?}
        
        RETRY[ğŸ”„ Retry File<br/>Increment attempts<br/>Enhance prompt feedback<br/>SAME worktree] --> GEN_PROMPT
        
        TEST_CHECK -->|Yes| RUN_TESTS
        TEST_CHECK -->|No| EXTRACT
        
        RUN_TESTS[ğŸ§ª Run Test Suite<br/>Location: IN worktree<br/>Mode: Full suite default<br/>Validate no breaks] --> TEST_OK{Tests<br/>passed?}
        
        TEST_OK -->|Failed| ERROR
        TEST_OK -->|Passed| EXTRACT
        
        EXTRACT[ğŸ“¤ Extract Diff<br/>Location: FROM worktree<br/>Method: GitPython<br/>Command: git diff HEAD<br/>Store in current_diff] --> APPLY
        
        APPLY[ğŸ”€ Apply to Main<br/>Location: MAIN workspace<br/>Method: GitPython git apply<br/>LOCK: Parallel safety<br/>Apply current_diff] --> COMMIT
        
        COMMIT[ğŸ’¾ Commit in Main<br/>Location: MAIN workspace<br/>Method: GitPython<br/>Message: Conventional<br/>Stage & commit file] --> DESTROY
        
        DESTROY[ğŸ—‘ï¸ Destroy Worktree<br/>Remove worktree<br/>Delete branch<br/>IMMEDIATELY<br/>Clear sandbox_path] --> RECORD
        
        RECORD[âœ… Record Success<br/>Add to successful_fixes<br/>Update ErrorMapper<br/>Clear current_prompt/diff] --> MORE{More files<br/>in queue?}
        
        ERROR[âŒ Handle Error<br/>Log failure details<br/>Add to failed_fixes<br/>Record in ErrorMapper] --> DESTROY_ERR
        
        DESTROY_ERR[ğŸ—‘ï¸ Destroy on Error<br/>Cleanup failed worktree<br/>Clear state] --> CONTINUE{Continue<br/>with others?}
        
        CONTINUE -->|Yes, has more| MORE
        CONTINUE -->|No, abort| FINAL_CLEANUP
        
        MORE -->|Next file| CREATE_WT
        MORE -->|All done| FINAL_CLEANUP
    end
    
    FINAL_CLEANUP[ğŸ§¹ Final Cleanup<br/>Save ErrorMapper learning<br/>Verify no worktrees left<br/>Generate summary<br/>Set final status] --> END([ğŸ‰ END])
    
    style START fill:#c8e6c9
    style END fill:#ffcdd2
    style CREATE_WT fill:#bbdefb
    style DESTROY fill:#ef9a9a
    style DESTROY_ERR fill:#ef9a9a
    style APPLY fill:#fff9c4
    style COMMIT fill:#c5e1a5
    style EXTRACT fill:#b2ebf2
    style CALL_AGENT fill:#ffe0b2
    style GEN_PROMPT fill:#e1bee7
    style RUN_TESTS fill:#b2dfdb
    style VERIFY fill:#f8bbd0
    style ERROR fill:#ffccbc
    style RETRY fill:#fff59d
    style RECORD fill:#dcedc8
    style FINAL_CLEANUP fill:#d1c4e9
```

### Detailed Text Flowchart - Step by Step

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STOMPER WORKFLOW - DETAILED FLOW                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[START]
  â”‚
  â”œâ”€â–º 1. INITIALIZE SESSION
  â”‚      â”œâ”€ Create session ID: stomper-YYYYMMDD-HHMMSS
  â”‚      â”œâ”€ Setup session state (files=[], current_file_index=0)
  â”‚      â”œâ”€ Initialize components (AgentManager, PromptGenerator, etc)
  â”‚      â””â”€ NO WORKTREE CREATED YET âœ…
  â”‚
  â”œâ”€â–º 2. COLLECT ALL ERRORS
  â”‚      â”œâ”€ Location: MAIN WORKSPACE (project_root)
  â”‚      â”œâ”€ Run quality tools: ruff, mypy, pytest
  â”‚      â”œâ”€ Parse JSON outputs
  â”‚      â”œâ”€ Group errors by file path
  â”‚      â”œâ”€ Create FileState for each file with errors
  â”‚      â”œâ”€ Build queue: files = [FileState(auth.py), FileState(models.py), ...]
  â”‚      â””â”€ Set current_file_index = 0
  â”‚
  â”œâ”€â–º 3. CHECK: Files to Process?
  â”‚      â”œâ”€ If NO files â†’ Jump to FINAL CLEANUP
  â”‚      â””â”€ If YES â†’ Continue to file processing loop
  â”‚
  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   â”‚         FILE PROCESSING LOOP (Repeat for Each File)          â”‚
  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚   
  â”œâ”€â–º 4. CREATE WORKTREE (for current file)
  â”‚      â”œâ”€ Get current file: files[current_file_index]
  â”‚      â”œâ”€ Generate worktree ID: {session_id}_{file.stem}
  â”‚      â”‚    Example: stomper-20251008-153000_auth
  â”‚      â”œâ”€ Create worktree using GitPython:
  â”‚      â”‚    repo.git.worktree("add", path, "-b", "sbx/{worktree_id}", "HEAD")
  â”‚      â”œâ”€ Location: .stomper/sandboxes/{worktree_id}
  â”‚      â”œâ”€ Store in state: sandbox_path = worktree_path
  â”‚      â””â”€ Log: "ğŸ—ï¸ Created worktree for auth.py"
  â”‚
  â”œâ”€â–º 5. GENERATE PROMPT (for this file)
  â”‚      â”œâ”€ Read file from worktree: worktree_path / file_path
  â”‚      â”œâ”€ Extract ONLY this file's errors from FileState
  â”‚      â”œâ”€ Convert ErrorInfo â†’ QualityError objects
  â”‚      â”œâ”€ Load error-specific advice from errors/ directory
  â”‚      â”œâ”€ Apply adaptive strategy based on retry_count
  â”‚      â”œâ”€ Render Jinja2 template (fix_prompt.j2)
  â”‚      â”œâ”€ Store in state: current_prompt = rendered_prompt
  â”‚      â””â”€ Log: "ğŸ“ Generated prompt (1234 chars)"
  â”‚
  â”œâ”€â–º 6. CALL AI AGENT (fix the file)
  â”‚      â”œâ”€ Location: IN WORKTREE (sandbox_path)
  â”‚      â”œâ”€ Increment: current_file.attempts += 1
  â”‚      â”œâ”€ Prepare context: {file_path, error_count, ...}
  â”‚      â”œâ”€ Call: agent.generate_fix_with_intelligent_fallback()
  â”‚      â”‚    â”œâ”€ Uses: current_prompt from previous step
  â”‚      â”‚    â”œâ”€ Uses: ErrorMapper for strategy selection
  â”‚      â”‚    â”œâ”€ Returns: fixed_code (complete file content)
  â”‚      â”‚    â””â”€ Records: outcome in ErrorMapper
  â”‚      â”œâ”€ Write fix: (worktree_path / file_path).write_text(fixed_code)
  â”‚      â””â”€ Log: "âœ… Agent completed fix"
  â”‚
  â”œâ”€â–º 7. VERIFY FIXES (in worktree)
  â”‚      â”œâ”€ Location: IN WORKTREE
  â”‚      â”œâ”€ Re-run quality tools on THIS file
  â”‚      â”œâ”€ Compare: original errors vs new errors
  â”‚      â”œâ”€ Calculate: fixed_errors = original - new
  â”‚      â”œâ”€ Update: current_file.fixed_errors
  â”‚      â”œâ”€ Update: current_file.errors (remaining)
  â”‚      â”œâ”€ Update: state.total_errors_fixed += len(fixed)
  â”‚      â””â”€ Log: "Fixed 2 errors, 0 remaining"
  â”‚
  â”œâ”€â–º 8. DECISION: Should Retry?
  â”‚      â”œâ”€ If ALL FIXED â†’ Continue to tests âœ…
  â”‚      â”œâ”€ If SOME REMAIN + attempts < max_retries:
  â”‚      â”‚    â””â”€ Go to RETRY (step 6b)
  â”‚      â””â”€ If MAX RETRIES â†’ Go to ERROR HANDLER
  â”‚
  â”œâ”€â–º 6b. RETRY (if needed)
  â”‚      â”œâ”€ Status: RETRYING
  â”‚      â”œâ”€ Add feedback to prompt context
  â”‚      â”œâ”€ KEEP SAME WORKTREE (don't recreate)
  â”‚      â””â”€ Loop back to: GENERATE PROMPT (step 5)
  â”‚           â”œâ”€ Retry count incremented
  â”‚           â””â”€ Adaptive strategy escalates (NORMAL â†’ DETAILED â†’ VERBOSE)
  â”‚
  â”œâ”€â–º 9. DECISION: Run Tests?
  â”‚      â”œâ”€ If test_validation enabled â†’ Continue to tests
  â”‚      â””â”€ If disabled â†’ Skip to EXTRACT DIFF
  â”‚
  â”œâ”€â–º 10. RUN TEST SUITE (in worktree)
  â”‚       â”œâ”€ Location: IN WORKTREE
  â”‚       â”œâ”€ Mode: Full test suite (default)
  â”‚       â”‚    Or: Quick mode (file-specific tests)
  â”‚       â”œâ”€ Execute: pytest via QualityToolManager
  â”‚       â”œâ”€ Check: Any test failures?
  â”‚       â”œâ”€ If PASS â†’ Continue âœ…
  â”‚       â””â”€ If FAIL â†’ Go to ERROR HANDLER âŒ
  â”‚
  â”œâ”€â–º 11. EXTRACT DIFF (from worktree)
  â”‚       â”œâ”€ Location: FROM worktree
  â”‚       â”œâ”€ Method: GitPython
  â”‚       â”œâ”€ Code: worktree_repo = Repo(sandbox_path)
  â”‚       â”‚        diff = worktree_repo.git.diff("HEAD")
  â”‚       â”œâ”€ Validation: Check diff is not empty
  â”‚       â”œâ”€ Store: state["current_diff"] = diff_content
  â”‚       â””â”€ Log: "ğŸ“¤ Diff extracted (567 bytes)"
  â”‚
  â”œâ”€â–º 12. APPLY TO MAIN WORKSPACE
  â”‚       â”œâ”€ Location: MAIN WORKSPACE (project_root)
  â”‚       â”œâ”€ ğŸ”’ LOCK: Acquire diff_application_lock (parallel safety)
  â”‚       â”œâ”€ Method: GitPython
  â”‚       â”œâ”€ Code: main_repo = Repo(project_root)
  â”‚       â”‚        with tempfile: write diff to .patch file
  â”‚       â”‚        main_repo.git.apply(patch_file)
  â”‚       â”œâ”€ Validation: Check apply succeeded
  â”‚       â”œâ”€ ğŸ”“ UNLOCK: Release diff_application_lock
  â”‚       â””â”€ Log: "âœ… Patch applied to main workspace"
  â”‚
  â”œâ”€â–º 13. COMMIT IN MAIN WORKSPACE
  â”‚       â”œâ”€ Location: MAIN WORKSPACE
  â”‚       â”œâ”€ Method: GitPython
  â”‚       â”œâ”€ Generate: Conventional commit message
  â”‚       â”‚    Format: fix(quality): resolve N issues in file.py
  â”‚       â”‚            - E501: ...
  â”‚       â”‚            - F401: ...
  â”‚       â”‚            Fixed by: stomper vX.Y.Z
  â”‚       â”œâ”€ Code: main_repo.index.add([file_path])
  â”‚       â”‚        main_repo.index.commit(commit_msg)
  â”‚       â”œâ”€ Record: successful_fixes.append(file_path)
  â”‚       â””â”€ Log: "ğŸ’¾ Committed in main workspace"
  â”‚
  â”œâ”€â–º 14. DESTROY WORKTREE (immediate cleanup)
  â”‚       â”œâ”€ Extract: worktree_id from sandbox_path.name
  â”‚       â”œâ”€ Method: GitPython via SandboxManager
  â”‚       â”œâ”€ Code: sandbox_manager.cleanup_sandbox(worktree_id)
  â”‚       â”‚        â”œâ”€ git worktree remove --force
  â”‚       â”‚        â””â”€ git branch -D sbx/{worktree_id}
  â”‚       â”œâ”€ Clear: state["sandbox_path"] = None
  â”‚       â”œâ”€ Clear: state["current_diff"] = None
  â”‚       â”œâ”€ Clear: state["current_prompt"] = None
  â”‚       â””â”€ Log: "ğŸ—‘ï¸ Worktree destroyed"
  â”‚       
  â”‚       â±ï¸ Worktree lifetime: ~10-30 seconds âœ…
  â”‚
  â”œâ”€â–º 15. RECORD SUCCESS
  â”‚       â”œâ”€ Update ErrorMapper with success/failure
  â”‚       â”œâ”€ Increment counters
  â”‚       â””â”€ Prepare for next file
  â”‚
  â”œâ”€â–º 16. DECISION: More Files?
  â”‚       â”œâ”€ If current_file_index + 1 < len(files):
  â”‚       â”‚    â”œâ”€ Increment current_file_index
  â”‚       â”‚    â””â”€ Loop back to: CREATE WORKTREE (step 4)
  â”‚       â”‚         â””â”€ Creates NEW worktree for next file âœ…
  â”‚       â””â”€ If all files processed:
  â”‚            â””â”€ Continue to: FINAL CLEANUP
  â”‚
  â”œâ”€â–º ERROR HANDLER (on any failure)
  â”‚       â”œâ”€ Log error details
  â”‚       â”œâ”€ Add to failed_fixes
  â”‚       â”œâ”€ Update ErrorMapper (failure outcome)
  â”‚       â”œâ”€ DESTROY WORKTREE (cleanup failed attempt)
  â”‚       â””â”€ Decision: Continue with other files?
  â”‚            â”œâ”€ If YES â†’ Increment index, loop to CREATE WORKTREE
  â”‚            â””â”€ If NO â†’ Continue to FINAL CLEANUP
  â”‚
  â””â”€â–º FINAL CLEANUP
         â”œâ”€ Save ErrorMapper learning data to disk
         â”œâ”€ Verify no worktrees remaining (sanity check)
         â”œâ”€ Generate session summary:
         â”‚    â”œâ”€ Files fixed: N
         â”‚    â”œâ”€ Files failed: M
         â”‚    â””â”€ Total errors fixed: X
         â”œâ”€ Set final status: COMPLETED or FAILED
         â””â”€ [END] ğŸ‰

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

KEY POINTS:
âœ… Worktree created FRESH for each file
âœ… Worktree destroyed IMMEDIATELY after use
âœ… Diff applied to MAIN workspace (not committed in worktree)
âœ… Commits happen in MAIN workspace
âœ… Lock ensures parallel safety for diff application
âœ… Full test suite runs by default (configurable)
âœ… GitPython used throughout (no subprocess)
```

---

## ğŸ“‹ Complete Implementation Checklist

### âœ… Phase 1: State & Foundation (1-2 hours)

**File: `src/stomper/workflow/state.py`**

- [ ] **Task 1.1: Add new state fields**
  - [ ] Add `current_prompt: str` to StomperState
  - [ ] Add `current_diff: str | None` to StomperState
  - [ ] Add `current_worktree_id: str | None` to StomperState
  - [ ] Add `test_validation: str` to StomperState
  - [ ] Add `continue_on_error: bool` to StomperState

**File: `src/stomper/workflow/orchestrator.py`**

- [ ] **Task 1.2: Add helper methods**
  - [ ] Implement `_get_diff_lock()` â†’ Returns asyncio.Lock
  - [ ] Add `_diff_application_lock` to __init__
  - [ ] Implement `_check_files_to_process()` â†’ Returns "has_files" or "no_files"
  - [ ] Implement `_should_continue_after_error()` â†’ Returns "continue" or "abort"

- [ ] **Task 1.3: Update imports**
  - [ ] Add `import asyncio`
  - [ ] Add `import tempfile`
  - [ ] Verify `from git import Repo` present

---

### âœ… Phase 2: New Workflow Nodes (2-3 hours)

**File: `src/stomper/workflow/orchestrator.py`**

- [ ] **Task 2.1: Implement create_worktree node**
  - [ ] Create `async def _create_worktree(state)` method
  - [ ] Generate worktree_id: `{session_id}_{file.stem}`
  - [ ] Call `sandbox_manager.create_sandbox(worktree_id, "HEAD")`
  - [ ] Store in `state["sandbox_path"]`
  - [ ] Store in `state["current_worktree_id"]`
  - [ ] Add logging with emoji
  - [ ] Handle direct mode (use_sandbox=False)

- [ ] **Task 2.2: Implement generate_prompt node**
  - [ ] Create `async def _generate_prompt(state)` method
  - [ ] Get current file from state
  - [ ] Read file content from worktree (or main if no sandbox)
  - [ ] Convert ErrorInfo â†’ QualityError list
  - [ ] Call `prompt_generator.generate_prompt(errors, context, retry_count)`
  - [ ] Store in `state["current_prompt"]`
  - [ ] Add logging

- [ ] **Task 2.3: Implement call_agent node**
  - [ ] Create `async def _call_agent(state)` method
  - [ ] Get current_prompt from state
  - [ ] Increment current_file.attempts
  - [ ] Prepare error_context dict
  - [ ] Call `agent_manager.generate_fix_with_intelligent_fallback()`
  - [ ] Write fixed_code to file in worktree
  - [ ] Add comprehensive error handling
  - [ ] Add logging

- [ ] **Task 2.4: Implement extract_diff node**
  - [ ] Create `async def _extract_diff(state)` method
  - [ ] Get sandbox_path from state
  - [ ] Use GitPython: `Repo(sandbox_path).git.diff("HEAD")`
  - [ ] Validate diff is not empty
  - [ ] Store in `state["current_diff"]`
  - [ ] Handle no-sandbox mode
  - [ ] Add logging

- [ ] **Task 2.5: Implement apply_to_main node**
  - [ ] Create `async def _apply_to_main(state)` method
  - [ ] Get current_diff from state
  - [ ] Acquire `self._diff_application_lock` (async with)
  - [ ] Use GitPython: `Repo(project_root).git.apply(patch_file)`
  - [ ] Write diff to temp .patch file
  - [ ] Apply patch, clean up temp file
  - [ ] Release lock
  - [ ] Add comprehensive error handling
  - [ ] Add logging

- [ ] **Task 2.6: Implement commit_in_main node**
  - [ ] Create `async def _commit_in_main(state)` method
  - [ ] Generate conventional commit message
  - [ ] Use GitPython: `main_repo.index.add([file_path])`
  - [ ] Use GitPython: `main_repo.index.commit(message)`
  - [ ] Add to successful_fixes
  - [ ] Add error handling
  - [ ] Add logging

- [ ] **Task 2.7: Implement destroy_worktree node**
  - [ ] Create `async def _destroy_worktree(state)` method
  - [ ] Get current_worktree_id from state
  - [ ] Call `sandbox_manager.cleanup_sandbox(worktree_id)`
  - [ ] Clear `state["sandbox_path"] = None`
  - [ ] Clear `state["current_diff"] = None`
  - [ ] Clear `state["current_prompt"] = None`
  - [ ] Add error handling (log warning, continue)
  - [ ] Add logging

- [ ] **Task 2.8: Implement destroy_worktree_on_error node**
  - [ ] Create `async def _destroy_worktree_on_error(state)` method
  - [ ] Same as destroy_worktree but for error path
  - [ ] Ensure cleanup always happens
  - [ ] Add logging for error cleanup

---

### âœ… Phase 3: Update Existing Nodes (1-2 hours)

**File: `src/stomper/workflow/orchestrator.py`**

- [ ] **Task 3.1: Update _initialize_session**
  - [ ] REMOVE worktree creation code
  - [ ] Keep session_id generation
  - [ ] Set `sandbox_path = None` (created per file)
  - [ ] Update docstring
  - [ ] Verify logging

- [ ] **Task 3.2: Update _collect_all_errors**
  - [ ] FORCE working_dir = state["project_root"] (no sandbox fallback)
  - [ ] Remove `state.get("sandbox_path")` check
  - [ ] Update comment: "Run in MAIN workspace"
  - [ ] Verify file grouping logic
  - [ ] Update docstring

- [ ] **Task 3.3: Update _verify_file_fixes**
  - [ ] Keep as-is (already runs in worktree)
  - [ ] Verify it uses `state.get("sandbox_path")`
  - [ ] Update docstring if needed

- [ ] **Task 3.4: Update _run_test_suite**
  - [ ] Keep worktree execution logic
  - [ ] Add test_validation mode support
  - [ ] Implement: full, quick, final, none modes
  - [ ] Update docstring

- [ ] **Task 3.5: Update _cleanup_session**
  - [ ] REMOVE worktree cleanup code
  - [ ] Add sanity check (warn if sandbox_path still set)
  - [ ] Keep ErrorMapper save logic
  - [ ] Update summary logging
  - [ ] Update docstring

- [ ] **Task 3.6: Remove/refactor _process_current_file**
  - [ ] Logic now split into: generate_prompt + call_agent
  - [ ] Either remove or refactor into smaller pieces
  - [ ] Update any references

- [ ] **Task 3.7: Update _handle_processing_error**
  - [ ] Add error details logging
  - [ ] Record in ErrorMapper
  - [ ] Update to use destroy_worktree_on_error path

---

### âœ… Phase 4: Rebuild Graph Structure (1-2 hours)

**File: `src/stomper/workflow/orchestrator.py`**

- [ ] **Task 4.1: Update _build_graph node registration**
  - [ ] Add `create_worktree` node
  - [ ] Add `generate_prompt` node
  - [ ] Add `call_agent` node
  - [ ] Add `extract_diff` node
  - [ ] Add `apply_to_main` node
  - [ ] Add `commit_in_main` node
  - [ ] Add `destroy_worktree` node
  - [ ] Add `destroy_worktree_on_error` node
  - [ ] Update existing nodes
  - [ ] Remove old `process_file` node

- [ ] **Task 4.2: Update edge structure - main path**
  - [ ] `initialize` â†’ `collect_errors`
  - [ ] `collect_errors` â†’ CONDITIONAL(`check_files_to_process`)
  - [ ] Conditional routes: `has_files` â†’ `create_worktree`, `no_files` â†’ `cleanup`

- [ ] **Task 4.3: Update edge structure - per-file chain**
  - [ ] `create_worktree` â†’ `generate_prompt`
  - [ ] `generate_prompt` â†’ `call_agent`
  - [ ] `call_agent` â†’ `verify_fixes`
  - [ ] `verify_fixes` â†’ CONDITIONAL(`should_retry_fixes`)
  - [ ] Conditional routes: `retry` â†’ `generate_prompt`, `success` â†’ test check, `abort` â†’ `handle_error`

- [ ] **Task 4.4: Update edge structure - test chain**
  - [ ] Test check CONDITIONAL: enabled â†’ `run_tests`, disabled â†’ `extract_diff`
  - [ ] `run_tests` â†’ CONDITIONAL(`check_test_results`)
  - [ ] Conditional routes: `pass` â†’ `extract_diff`, `fail` â†’ `handle_error`

- [ ] **Task 4.5: Update edge structure - diff application chain**
  - [ ] `extract_diff` â†’ `apply_to_main`
  - [ ] `apply_to_main` â†’ `commit_in_main`
  - [ ] `commit_in_main` â†’ `destroy_worktree`
  - [ ] `destroy_worktree` â†’ CONDITIONAL(`check_more_files`)
  - [ ] Conditional routes: `next` â†’ `create_worktree`, `done` â†’ `cleanup`

- [ ] **Task 4.6: Update edge structure - error path**
  - [ ] `handle_error` â†’ `destroy_worktree_on_error`
  - [ ] `destroy_worktree_on_error` â†’ CONDITIONAL(`should_continue_after_error`)
  - [ ] Conditional routes: `continue` â†’ check more files, `abort` â†’ `cleanup`
  - [ ] `cleanup` â†’ `END`

- [ ] **Task 4.7: Update _should_retry_fixes conditional**
  - [ ] Change retry route: `retry` â†’ `generate_prompt` (not `retry_file`)
  - [ ] Keep other routes: `success`, `abort`

- [ ] **Task 4.8: Remove old edges**
  - [ ] Remove `process_file` related edges
  - [ ] Remove `retry_file` â†’ `process_file` edge
  - [ ] Clean up any orphaned edge references

---

### âœ… Phase 5: Configuration Updates (30 min)

**File: `src/stomper/config/models.py`**

- [ ] **Task 5.1: Update WorkflowConfig**
  - [ ] Add `test_validation: Literal["full", "quick", "final", "none"] = "full"`
  - [ ] Add `files_per_worktree: int = Field(default=1, ge=1)`
  - [ ] Add `continue_on_error: bool = True`
  - [ ] Update docstrings

**File: `src/stomper/workflow/state.py`**

- [ ] **Task 5.2: Add TestValidation enum**
  - [ ] Create `TestValidation(str, Enum)` class
  - [ ] Add: FULL, QUICK, FINAL, NONE values
  - [ ] Add docstrings

---

### âœ… Phase 6: Testing Updates (2-3 hours)

**File: `tests/e2e/test_workflow_integration.py`**

- [ ] **Task 6.1: Update existing tests**
  - [ ] Update `test_full_workflow_success` for new flow
  - [ ] Update `test_workflow_with_retry` for new flow
  - [ ] Update `test_workflow_test_validation` for new flow
  - [ ] Update `test_workflow_git_isolation` for per-file worktrees
  - [ ] Update `test_workflow_adaptive_learning` for new flow
  - [ ] Update `test_workflow_no_errors_found` for new flow

- [ ] **Task 6.2: Add new tests**
  - [ ] Add `test_worktree_per_file_lifecycle` - Verify one worktree per file
  - [ ] Add `test_diff_extraction_gitpython` - Verify GitPython usage
  - [ ] Add `test_diff_application_to_main` - Verify main workspace updates
  - [ ] Add `test_commit_in_main_workspace` - Verify commits in main
  - [ ] Add `test_worktree_immediate_destruction` - Verify cleanup timing
  - [ ] Add `test_parallel_diff_lock` - Verify lock prevents races

- [ ] **Task 6.3: Update test utilities**
  - [ ] Update MockAIAgent if needed
  - [ ] Add helpers for tracking worktree lifecycle
  - [ ] Add helpers for verifying git operations

---

### âœ… Phase 7: Documentation Updates (1 hour)

**File: Various documentation files**

- [ ] **Task 7.1: Update task-6-IMPLEMENTATION-COMPLETE.md**
  - [ ] Update mermaid diagram to show per-file worktrees
  - [ ] Update architecture description
  - [ ] Update node descriptions

- [ ] **Task 7.2: Update task-6-FINAL-REVIEW.md**
  - [ ] Update workflow description
  - [ ] Update verification points

- [ ] **Task 7.3: Create refactoring summary**
  - [ ] Document what changed
  - [ ] Document why it changed
  - [ ] Document migration notes

---

### âœ… Phase 8: Verification & Testing (1-2 hours)

**Testing suite verification**

- [ ] **Task 8.1: Run unit tests**
  - [ ] Run: `uv run pytest tests/unit/ -v`
  - [ ] Verify: 267+ tests pass
  - [ ] Fix any broken tests
  - [ ] Verify: 0 linting errors

- [ ] **Task 8.2: Run E2E tests**
  - [ ] Run: `uv run pytest tests/e2e/test_workflow_integration.py -k asyncio -v`
  - [ ] Verify: All workflow tests pass
  - [ ] Add any missing edge case tests
  - [ ] Verify: New lifecycle tests pass

- [ ] **Task 8.3: Manual verification**
  - [ ] Create test project with 2-3 files with errors
  - [ ] Run workflow manually
  - [ ] Verify: Separate worktree for each file
  - [ ] Verify: Worktrees destroyed immediately
  - [ ] Verify: Diffs applied to main workspace
  - [ ] Verify: Commits in main workspace
  - [ ] Verify: No worktrees remain after session

- [ ] **Task 8.4: Linting & type checking**
  - [ ] Run: `uv run ruff check src/stomper/workflow/`
  - [ ] Run: `uv run mypy src/stomper/workflow/`
  - [ ] Fix any issues
  - [ ] Verify: 0 errors

---

## ğŸ¯ Overview

### What Needs to Change

The current implementation creates **one worktree for the entire session** and processes all files within it. 

**Your actual design:** Create **one worktree per file**, process it completely, apply diff to main workspace, then destroy the worktree immediately.

### Why This Is Better

1. âœ… **True isolation** - Each file gets fresh environment
2. âœ… **Parallel-ready** - Can run multiple worktrees simultaneously
3. âœ… **Faster cleanup** - Worktree destroyed immediately after use
4. âœ… **Smaller diffs** - Each diff is just one file's changes
5. âœ… **Better debugging** - Know exactly which file caused issues
6. âœ… **Atomic operations** - Apply/commit/destroy as one unit

---

## ğŸ“Š Current vs. Target Architecture

### Current Implementation (INCORRECT)

```mermaid
flowchart TD
    START([START]) --> init["Initialize<br/>CREATE worktree once"]
    init --> collect["Collect Errors<br/>All files"]
    collect --> loop_start["Start File Loop"]
    
    loop_start --> process["Process File<br/>IN SAME WORKTREE"]
    process --> verify["Verify<br/>IN SAME WORKTREE"]
    verify --> test["Test<br/>IN SAME WORKTREE"]
    test --> commit["Commit<br/>IN SAME WORKTREE"]
    
    commit --> more{More?}
    more -->|Yes| loop_start
    more -->|No| cleanup["Cleanup<br/>DESTROY worktree"]
    cleanup --> END([END])
    
    style init fill:#ffcdd2
    style cleanup fill:#ffcdd2
```

**Problem:** All files processed in ONE worktree! âŒ

---

### Target Implementation (CORRECT - Your Design)

```mermaid
flowchart TD
    START([START]) --> init["ğŸ“‹ Initialize Session<br/>- Create session ID<br/>- NO worktree yet"]
    
    init --> collect["ğŸ” Collect Errors<br/>- Run quality tools in MAIN workspace<br/>- Group errors by file<br/>- Create file queue"]
    
    collect --> check_files{Files with<br/>errors?}
    check_files -->|No| final_cleanup
    check_files -->|Yes| create_wt
    
    create_wt["ğŸ—ï¸ Create Worktree<br/>- For CURRENT file only<br/>- Branch: sbx/session_file<br/>- Base: HEAD<br/>- FRESH checkout"]
    
    create_wt --> gen_prompt["ğŸ“ Generate Prompt<br/>- Extract THIS file's errors only<br/>- Load error advice<br/>- Adaptive strategy<br/>- File-specific context"]
    
    gen_prompt --> call_agent["ğŸ¤– Call Agent<br/>- In worktree<br/>- Fix THIS file only<br/>- Agent returns control"]
    
    call_agent --> verify["ğŸ” Verify Fixes<br/>- Re-run quality tools<br/>- In worktree<br/>- On THIS file"]
    
    verify --> fixed{All<br/>fixed?}
    
    fixed -->|No, retry < max| retry["ğŸ”„ Retry<br/>- Same worktree<br/>- Enhanced prompt"]
    retry --> gen_prompt
    
    fixed -->|No, max retries| error_handler
    fixed -->|Yes| run_tests
    
    run_tests["ğŸ§ª Run Tests<br/>- FULL test suite<br/>- In worktree<br/>- Ensure nothing broke"]
    
    run_tests --> tests_ok{Tests<br/>passed?}
    
    tests_ok -->|No| error_handler
    tests_ok -->|Yes| extract
    
    extract["ğŸ“¤ Extract Diff<br/>- git diff HEAD<br/>- In worktree<br/>- Using GitPython"]
    
    extract --> apply["ğŸ”€ Apply to Main<br/>- LOCK for parallel safety<br/>- git apply patch<br/>- In MAIN workspace<br/>- Using GitPython"]
    
    apply --> commit_main["ğŸ’¾ Commit in Main<br/>- Conventional message<br/>- In MAIN workspace<br/>- Using GitPython"]
    
    commit_main --> destroy["ğŸ—‘ï¸ Destroy Worktree<br/>- git worktree remove<br/>- git branch -D<br/>- IMMEDIATELY<br/>- Using GitPython"]
    
    destroy --> record_success["âœ… Record Success<br/>- Add to successful_fixes<br/>- Update ErrorMapper"]
    
    record_success --> more{More<br/>files?}
    
    more -->|Yes| create_wt
    more -->|No| final_cleanup
    
    error_handler["âŒ Handle Error<br/>- Record failure<br/>- Destroy worktree<br/>- Continue or abort?"]
    
    error_handler --> destroy_error["ğŸ—‘ï¸ Destroy Worktree<br/>- Clean up failed attempt"]
    
    destroy_error --> continue{Continue<br/>others?}
    
    continue -->|Yes| more
    continue -->|No| final_cleanup
    
    final_cleanup["ğŸ§¹ Final Cleanup<br/>- Save ErrorMapper data<br/>- Generate summary<br/>- NO worktrees left!"]
    
    final_cleanup --> END([END])
    
    style create_wt fill:#e3f2fd
    style destroy fill:#ffcdd2
    style destroy_error fill:#ffcdd2
    style apply fill:#fff3e0
    style commit_main fill:#e8f5e9
```

**Key Difference:** Worktree lifecycle is **per file**, not per session! âœ…

---

## ğŸ”„ Detailed Flow Comparison

### Current Flow (Session-Level Worktree)

```
Session Start
    â†“
Create ONE worktree
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Process auth.py     â”‚ â† In same worktree
â”‚ Process models.py   â”‚ â† In same worktree
â”‚ Process utils.py    â”‚ â† In same worktree
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Destroy ONE worktree
    â†“
Session End
```

**Timeline:** One long-lived worktree (minutes)

---

### Target Flow (File-Level Worktree)

```
Session Start
    â†“
Collect all errors (in main workspace)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Create worktree_auth             â”‚
â”‚   â†“                               â”‚
â”‚ Fix auth.py                       â”‚
â”‚   â†“                               â”‚
â”‚ Verify in worktree_auth           â”‚
â”‚   â†“                               â”‚
â”‚ Test in worktree_auth             â”‚
â”‚   â†“                               â”‚
â”‚ Extract diff from worktree_auth   â”‚
â”‚   â†“                               â”‚
â”‚ Apply diff to MAIN workspace      â”‚
â”‚   â†“                               â”‚
â”‚ Commit in MAIN workspace          â”‚
â”‚   â†“                               â”‚
â”‚ Destroy worktree_auth             â”‚ â† Immediate cleanup!
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Create worktree_models           â”‚ â† NEW worktree!
â”‚   â†“                               â”‚
â”‚ Fix models.py                     â”‚
â”‚   â†“                               â”‚
â”‚ Verify in worktree_models         â”‚
â”‚   â†“                               â”‚
â”‚ Test in worktree_models           â”‚
â”‚   â†“                               â”‚
â”‚ Extract diff from worktree_models â”‚
â”‚   â†“                               â”‚
â”‚ Apply diff to MAIN workspace      â”‚
â”‚   â†“                               â”‚
â”‚ Commit in MAIN workspace          â”‚
â”‚   â†“                               â”‚
â”‚ Destroy worktree_models           â”‚ â† Immediate cleanup!
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Session End (no worktrees left!)
```

**Timeline:** Multiple short-lived worktrees (~30 sec each)

---

## ğŸ“ Required Changes

### Change 1: Split collect_errors Node

**Current:**
```python
async def _collect_all_errors(self, state: StomperState):
    # Runs in worktree (WRONG!)
    working_dir = state.get("sandbox_path") or state["project_root"]
```

**Target:**
```python
async def _collect_all_errors(self, state: StomperState):
    # ALWAYS runs in main workspace (no worktree yet!)
    working_dir = state["project_root"]
    
    # Collect all errors
    all_errors = self.quality_manager.run_tools(...)
    
    # Group by file
    files_with_errors = self._group_errors_by_file(all_errors)
    
    # Create file queue
    state["files"] = files_with_errors
    state["current_file_index"] = 0
    
    return state
```

---

### Change 2: Remove Worktree from Initialize

**Current:**
```python
async def _initialize_session(self, state: StomperState):
    # Creates worktree (WRONG!)
    if self.use_sandbox:
        sandbox_path = self.sandbox_manager.create_sandbox(session_id)
```

**Target:**
```python
async def _initialize_session(self, state: StomperState):
    session_id = f"stomper-{datetime.now().strftime('%Y%m%d-%H%M%S')}"
    
    # NO worktree creation!
    state.update({
        "session_id": session_id,
        "sandbox_path": None,  # Will be created per file
        "status": ProcessingStatus.IN_PROGRESS,
    })
    
    return state
```

---

### Change 3: Add create_worktree Node

**New Node:**
```python
async def _create_worktree(self, state: StomperState) -> StomperState:
    """Create worktree for current file."""
    current_file = state["files"][state["current_file_index"]]
    session_id = state["session_id"]
    
    # Create unique worktree for THIS file
    file_stem = current_file.file_path.stem
    worktree_id = f"{session_id}_{file_stem}"
    
    logger.info(f"ğŸ—ï¸ Creating worktree for {current_file.file_path}")
    
    if self.use_sandbox:
        # Create worktree using GitPython
        sandbox_path = self.sandbox_manager.create_sandbox(
            session_id=worktree_id,
            base_branch="HEAD"
        )
        state["sandbox_path"] = sandbox_path
        logger.info(f"âœ… Worktree created: {sandbox_path}")
    else:
        state["sandbox_path"] = None
        logger.info("âš ï¸ Running in direct mode (no sandbox)")
    
    return state
```

---

### Change 4: Add generate_prompt Node

**New Node:**
```python
async def _generate_prompt(self, state: StomperState) -> StomperState:
    """Generate AI prompt for current file."""
    current_file = state["files"][state["current_file_index"]]
    working_dir = state.get("sandbox_path") or state["project_root"]
    
    logger.info(f"ğŸ“ Generating prompt for {current_file.file_path}")
    
    # Read file content from worktree
    file_path = working_dir / current_file.file_path
    code_context = file_path.read_text()
    
    # Convert ErrorInfo to QualityError
    from stomper.quality.base import QualityError
    quality_errors = [
        QualityError(
            tool=err.tool,
            file=err.file_path,
            line=err.line_number,
            column=err.column or 0,
            code=err.code,
            message=err.message,
            severity=err.severity,
            auto_fixable=err.auto_fixable,
        )
        for err in current_file.errors
    ]
    
    # Generate prompt with adaptive strategy
    prompt = state["prompt_generator"].generate_prompt(
        errors=quality_errors,
        code_context=code_context,
        retry_count=current_file.attempts,
    )
    
    # Store prompt in state for agent node
    state["current_prompt"] = prompt
    
    logger.debug(f"Generated prompt ({len(prompt)} chars)")
    
    return state
```

---

### Change 5: Split process_file into call_agent Node

**Current (Too much):**
```python
async def _process_current_file(self, state: StomperState):
    # Reads file
    # Generates prompt  
    # Calls agent
    # Applies fix
    # ALL IN ONE!
```

**Target:**
```python
async def _call_agent(self, state: StomperState) -> StomperState:
    """Call AI agent to fix current file."""
    current_file = state["files"][state["current_file_index"]]
    working_dir = state.get("sandbox_path") or state["project_root"]
    
    logger.info(f"ğŸ¤– Calling agent to fix {current_file.file_path}")
    
    current_file.attempts += 1
    
    try:
        # Prepare context for agent
        error_context = {
            "file_path": str(current_file.file_path),
            "error_count": len(current_file.errors),
        }
        
        code_context = (working_dir / current_file.file_path).read_text()
        
        # Call agent with intelligent fallback
        fixed_code = state["agent_manager"].generate_fix_with_intelligent_fallback(
            primary_agent_name="cursor-cli",
            error=current_file.errors[0],  # Primary error (QualityError)
            error_context=error_context,
            code_context=code_context,
            prompt=state["current_prompt"],  # From previous node
            max_retries=1,
        )
        
        # Apply fix in worktree
        file_path = working_dir / current_file.file_path
        file_path.write_text(fixed_code)
        
        logger.info(f"âœ… Agent completed fix for {current_file.file_path}")
        
    except Exception as e:
        logger.error(f"âŒ Agent failed: {e}")
        current_file.last_error = str(e)
        raise  # Let error handler deal with it
    
    return state
```

---

### Change 6: Add extract_diff Node

**New Node:**
```python
async def _extract_diff(self, state: StomperState) -> StomperState:
    """Extract diff from worktree."""
    current_file = state["files"][state["current_file_index"]]
    sandbox_path = state.get("sandbox_path")
    
    if not sandbox_path:
        logger.info("No sandbox - skipping diff extraction")
        state["current_diff"] = None
        return state
    
    logger.info(f"ğŸ“¤ Extracting diff for {current_file.file_path}")
    
    try:
        # Use GitPython to get diff
        from git import Repo
        
        worktree_repo = Repo(sandbox_path)
        diff_content = worktree_repo.git.diff("HEAD")
        
        if not diff_content:
            logger.warning("No changes detected in worktree")
            state["current_diff"] = None
        else:
            state["current_diff"] = diff_content
            logger.info(f"âœ… Diff extracted ({len(diff_content)} bytes)")
        
    except Exception as e:
        logger.error(f"âŒ Failed to extract diff: {e}")
        raise
    
    return state
```

---

### Change 7: Add apply_to_main Node

**New Node with Parallel Safety:**
```python
async def _apply_to_main(self, state: StomperState) -> StomperState:
    """Apply diff to main workspace with parallel safety lock."""
    current_file = state["files"][state["current_file_index"]]
    diff_content = state.get("current_diff")
    
    if not diff_content:
        logger.info("No diff to apply")
        return state
    
    logger.info(f"ğŸ”€ Applying diff to main workspace for {current_file.file_path}")
    
    # Use lock for parallel safety (Phase 2)
    async with self._get_diff_lock():
        try:
            from git import Repo
            import tempfile
            
            main_repo = Repo(self.project_root)
            
            # Write diff to temp file
            with tempfile.NamedTemporaryFile(mode='w', suffix='.patch', delete=False) as f:
                f.write(diff_content)
                patch_file = f.name
            
            try:
                # Apply patch using GitPython
                main_repo.git.apply(patch_file)
                logger.info(f"âœ… Patch applied to main workspace")
                
            finally:
                # Clean up temp file
                Path(patch_file).unlink()
            
        except Exception as e:
            logger.error(f"âŒ Failed to apply patch: {e}")
            raise
    
    return state

def _get_diff_lock(self):
    """Get lock for diff application (creates if needed)."""
    if not hasattr(self, '_diff_application_lock'):
        import asyncio
        self._diff_application_lock = asyncio.Lock()
    return self._diff_application_lock
```

---

### Change 8: Add commit_in_main Node

**New Node:**
```python
async def _commit_in_main(self, state: StomperState) -> StomperState:
    """Commit changes in main workspace."""
    current_file = state["files"][state["current_file_index"]]
    
    logger.info(f"ğŸ’¾ Committing changes in main workspace for {current_file.file_path}")
    
    try:
        from git import Repo
        
        main_repo = Repo(self.project_root)
        
        # Generate conventional commit message
        error_codes = [e.code for e in current_file.fixed_errors]
        commit_msg = (
            f"fix(quality): resolve {len(error_codes)} issues in {current_file.file_path.name}\n\n"
            + "\n".join(f"- {code}" for code in error_codes)
            + f"\n\nFixed by: stomper v{self._get_version()}"
        )
        
        # Stage and commit using GitPython
        main_repo.index.add([str(current_file.file_path)])
        main_repo.index.commit(commit_msg)
        
        logger.info(f"âœ… Committed in main workspace: {current_file.file_path}")
        
        # Record success
        state["successful_fixes"].append(str(current_file.file_path))
        
    except Exception as e:
        logger.error(f"âŒ Failed to commit: {e}")
        raise
    
    return state
```

---

### Change 9: Add destroy_worktree Node

**New Node:**
```python
async def _destroy_worktree(self, state: StomperState) -> StomperState:
    """Destroy worktree for current file."""
    sandbox_path = state.get("sandbox_path")
    
    if not sandbox_path:
        logger.debug("No worktree to destroy")
        return state
    
    current_file = state["files"][state["current_file_index"]]
    logger.info(f"ğŸ—‘ï¸ Destroying worktree for {current_file.file_path}")
    
    try:
        # Extract session ID from sandbox_path
        # Format: .stomper/sandboxes/{session_id}_{file_stem}
        session_id = sandbox_path.name
        
        # Cleanup using GitPython (via SandboxManager)
        self.sandbox_manager.cleanup_sandbox(session_id)
        
        # Clear from state
        state["sandbox_path"] = None
        
        logger.info(f"âœ… Worktree destroyed")
        
    except Exception as e:
        logger.warning(f"âš ï¸ Failed to destroy worktree: {e}")
        # Continue anyway - don't fail the workflow
    
    return state
```

---

### Change 10: Update Cleanup Node

**Current:**
```python
async def _cleanup_session(self, state: StomperState):
    # Destroys worktree (WRONG - should already be gone!)
    if self.sandbox_manager and state.get("sandbox_path"):
        self.sandbox_manager.cleanup_sandbox(state["session_id"])
```

**Target:**
```python
async def _cleanup_session(self, state: StomperState) -> StomperState:
    """Clean up session resources."""
    logger.info("ğŸ§¹ Final session cleanup")
    
    # Save mapper learning data
    if state.get("mapper"):
        try:
            state["mapper"].save()
            logger.info("âœ… Saved learning data")
        except Exception as e:
            logger.warning(f"Failed to save mapper data: {e}")
    
    # Verify no worktrees left (sanity check)
    if state.get("sandbox_path"):
        logger.warning("âš ï¸ Worktree still exists - this shouldn't happen!")
        # Try to clean it up anyway
        try:
            self.sandbox_manager.cleanup_sandbox(state["sandbox_path"].name)
        except Exception:
            pass
    
    # Final status
    if state["status"] != ProcessingStatus.FAILED:
        state["status"] = ProcessingStatus.COMPLETED
    
    # Generate summary
    logger.info(
        f"ğŸ‰ Session complete:\n"
        f"  - Files fixed: {len(state['successful_fixes'])}\n"
        f"  - Files failed: {len(state['failed_fixes'])}\n"
        f"  - Total errors fixed: {state['total_errors_fixed']}"
    )
    
    return state
```

---

### Change 11: Update Graph Structure

**Current edges:**
```python
workflow.add_edge("initialize", "collect_errors")
workflow.add_edge("collect_errors", "process_file")
workflow.add_edge("process_file", "verify_fixes")
```

**Target edges:**
```python
# Initialize â†’ Collect (no worktree yet)
workflow.add_edge("initialize", "collect_errors")

# Collect â†’ Check if files exist
workflow.add_conditional_edges(
    "collect_errors",
    self._check_files_to_process,
    {
        "has_files": "create_worktree",
        "no_files": "cleanup",
    }
)

# Per-File Processing Chain
workflow.add_edge("create_worktree", "generate_prompt")
workflow.add_edge("generate_prompt", "call_agent")
workflow.add_edge("call_agent", "verify_fixes")

# Verify â†’ Retry or Continue
workflow.add_conditional_edges(
    "verify_fixes",
    self._should_retry_fixes,
    {
        "retry": "generate_prompt",  # Re-generate with feedback
        "success": "run_tests",
        "abort": "handle_error",
    }
)

# Tests â†’ Extract Diff or Error
workflow.add_conditional_edges(
    "run_tests",
    self._check_test_results,
    {
        "pass": "extract_diff",
        "fail": "handle_error",
    }
)

# Diff Application Chain (all in main workspace)
workflow.add_edge("extract_diff", "apply_to_main")
workflow.add_edge("apply_to_main", "commit_in_main")
workflow.add_edge("commit_in_main", "destroy_worktree")

# After destroying worktree â†’ Check for more files
workflow.add_conditional_edges(
    "destroy_worktree",
    self._check_more_files,
    {
        "next": "create_worktree",  # Create NEW worktree for next file!
        "done": "cleanup",
    }
)

# Error handling also destroys worktree
workflow.add_edge("handle_error", "destroy_worktree_on_error")
workflow.add_conditional_edges(
    "destroy_worktree_on_error",
    self._should_continue_after_error,
    {
        "continue": "next_file",  # Will create new worktree
        "abort": "cleanup",
    }
)
```

---

## ğŸ—‚ï¸ File-by-File Implementation Plan

### Step 1: Update state.py (Add New Fields)

**Add to StomperState:**
```python
class StomperState(TypedDict, total=False):
    # Existing fields...
    
    # New fields for per-file processing
    current_prompt: str  # Generated prompt for current file
    current_diff: str | None  # Extracted diff for current file
    current_worktree_id: str | None  # ID of current worktree
```

---

### Step 2: Update orchestrator.py (Add New Nodes)

**Add these methods to StomperWorkflow:**

1. `_create_worktree()` - Create worktree for file
2. `_generate_prompt()` - Generate file-specific prompt
3. `_call_agent()` - Call agent (split from process_file)
4. `_extract_diff()` - Extract diff using GitPython
5. `_apply_to_main()` - Apply patch to main workspace
6. `_commit_in_main()` - Commit in main workspace
7. `_destroy_worktree()` - Destroy worktree immediately
8. `_destroy_worktree_on_error()` - Cleanup on error path
9. `_check_files_to_process()` - Check if any files to process
10. `_should_continue_after_error()` - Decide if continue after error
11. `_get_diff_lock()` - Get asyncio lock for parallel safety

**Remove/refactor:**
- `_process_current_file()` - Split into call_agent + others

---

### Step 3: Update _build_graph() (New Flow)

**Complete new graph structure:**
```python
def _build_graph(self) -> Any:
    """Build LangGraph state machine."""
    workflow = StateGraph(StomperState)
    
    # Add all nodes
    workflow.add_node("initialize", self._initialize_session)
    workflow.add_node("collect_errors", self._collect_all_errors)
    workflow.add_node("create_worktree", self._create_worktree)
    workflow.add_node("generate_prompt", self._generate_prompt)
    workflow.add_node("call_agent", self._call_agent)
    workflow.add_node("verify_fixes", self._verify_file_fixes)
    workflow.add_node("run_tests", self._run_test_suite)
    workflow.add_node("extract_diff", self._extract_diff)
    workflow.add_node("apply_to_main", self._apply_to_main)
    workflow.add_node("commit_in_main", self._commit_in_main)
    workflow.add_node("destroy_worktree", self._destroy_worktree)
    workflow.add_node("next_file", self._move_to_next_file)
    workflow.add_node("cleanup", self._cleanup_session)
    workflow.add_node("handle_error", self._handle_processing_error)
    workflow.add_node("destroy_worktree_on_error", self._destroy_worktree)
    
    # Set entry point
    workflow.set_entry_point("initialize")
    
    # Build edges (see Change 11 above for complete structure)
    # ... (detailed in implementation section)
    
    return workflow.compile()
```

---

### Step 4: Use GitPython Everywhere

**Replace subprocess calls with GitPython:**

**Before:**
```python
subprocess.run(["git", "diff", "HEAD"], cwd=worktree_path, ...)
subprocess.run(["git", "apply", patch_file], cwd=main_workspace, ...)
subprocess.run(["git", "commit", "-m", msg], cwd=main_workspace, ...)
```

**After:**
```python
from git import Repo

# Get diff
worktree_repo = Repo(worktree_path)
diff = worktree_repo.git.diff("HEAD")

# Apply patch
main_repo = Repo(main_workspace)
main_repo.git.apply(patch_file)

# Commit
main_repo.index.add([file_path])
main_repo.index.commit(commit_message)
```

**Already using GitPython in:**
- âœ… `sandbox_manager.py` - Worktree operations
- âœ… `discovery/git.py` - Git discovery
- âœ… `fix_applier.py` - Some operations

**Just need to be consistent!**

---

## ğŸ§ª Testing Changes

### Update test_workflow_integration.py

**Current tests assume:**
- One worktree for session

**Need to verify:**
- Worktree created per file
- Worktree destroyed immediately
- Multiple files = multiple worktrees
- Diff applied to main workspace
- Commits in main workspace

**New test:**
```python
@pytest.mark.e2e
@pytest.mark.anyio
async def test_worktree_per_file_lifecycle():
    """Test that each file gets its own worktree."""
    from stomper.workflow.orchestrator import StomperWorkflow
    
    # Setup project with 2 files with errors
    test_file1 = tmp_path / "src" / "auth.py"
    test_file1.parent.mkdir(parents=True)
    test_file1.write_text("import os\n")  # F401
    
    test_file2 = tmp_path / "src" / "models.py"
    test_file2.parent.mkdir(parents=True)
    test_file2.write_text("import sys\n")  # F401
    
    # Initialize git
    subprocess.run(["git", "init"], cwd=tmp_path, check=True)
    subprocess.run(["git", "config", "user.email", "test@test.com"], cwd=tmp_path)
    subprocess.run(["git", "config", "user.name", "Test"], cwd=tmp_path)
    subprocess.run(["git", "add", "."], cwd=tmp_path, check=True)
    subprocess.run(["git", "commit", "-m", "initial"], cwd=tmp_path, check=True)
    
    # Track worktree lifecycle
    worktrees_created = []
    worktrees_destroyed = []
    
    # Mock to track calls
    original_create = workflow.sandbox_manager.create_sandbox
    original_cleanup = workflow.sandbox_manager.cleanup_sandbox
    
    def track_create(session_id, base_branch="HEAD"):
        path = original_create(session_id, base_branch)
        worktrees_created.append(session_id)
        return path
    
    def track_destroy(session_id):
        worktrees_destroyed.append(session_id)
        original_cleanup(session_id)
    
    workflow.sandbox_manager.create_sandbox = track_create
    workflow.sandbox_manager.cleanup_sandbox = track_destroy
    
    # Run workflow
    test_agent = MockAIAgent(return_value="# Fixed\n")
    workflow = StomperWorkflow(project_root=tmp_path, use_sandbox=True, run_tests=False)
    workflow.register_agent("cursor-cli", test_agent)
    
    final_state = await workflow.run({"enabled_tools": ["ruff"]})
    
    # Verify lifecycle
    assert len(worktrees_created) == 2  # One per file!
    assert len(worktrees_destroyed) == 2  # Both destroyed!
    
    # Verify they were different worktrees
    assert worktrees_created[0] != worktrees_created[1]
    
    # Verify both files were fixed
    assert len(final_state["successful_fixes"]) == 2
```

---

## ğŸ“‹ Implementation Checklist

### Phase 1: State & Helper Methods (1-2 hours)

- [ ] Add `current_prompt` to StomperState
- [ ] Add `current_diff` to StomperState  
- [ ] Add `current_worktree_id` to StomperState
- [ ] Add `_get_diff_lock()` helper method
- [ ] Add `_check_files_to_process()` conditional function
- [ ] Add `_should_continue_after_error()` conditional function
- [ ] Update imports (add `asyncio.Lock`)

### Phase 2: New Nodes (2-3 hours)

- [ ] Implement `_create_worktree()` node
- [ ] Implement `_generate_prompt()` node
- [ ] Implement `_call_agent()` node (split from process_file)
- [ ] Implement `_extract_diff()` node
- [ ] Implement `_apply_to_main()` node with lock
- [ ] Implement `_commit_in_main()` node
- [ ] Implement `_destroy_worktree()` node
- [ ] Add `_destroy_worktree_on_error()` node

### Phase 3: Update Existing Nodes (1 hour)

- [ ] Update `_initialize_session()` - Remove worktree creation
- [ ] Update `_collect_all_errors()` - Always use main workspace
- [ ] Update `_verify_file_fixes()` - Keep as-is (runs in worktree)
- [ ] Update `_run_test_suite()` - Keep as-is (runs in worktree)
- [ ] Update `_cleanup_session()` - Remove worktree cleanup
- [ ] Remove old `_process_current_file()` or refactor it

### Phase 4: Rebuild Graph (1 hour)

- [ ] Update `_build_graph()` with new node structure
- [ ] Add `collect_errors` â†’ conditional â†’ `create_worktree` or `cleanup`
- [ ] Chain: `create_worktree` â†’ `generate_prompt` â†’ `call_agent` â†’ `verify_fixes`
- [ ] Update retry logic to go back to `generate_prompt` (not `process_file`)
- [ ] Chain: `run_tests` â†’ `extract_diff` â†’ `apply_to_main` â†’ `commit_in_main` â†’ `destroy_worktree`
- [ ] Update `more_files` to go to `create_worktree` (not `next_file` â†’ `process_file`)
- [ ] Add error path: `handle_error` â†’ `destroy_worktree_on_error` â†’ conditional

### Phase 5: Testing (2-3 hours)

- [ ] Update existing tests for new flow
- [ ] Add test for per-file worktree lifecycle
- [ ] Add test for diff extraction
- [ ] Add test for diff application to main
- [ ] Add test for GitPython usage
- [ ] Verify all 273 tests still pass
- [ ] Add test for parallel safety lock

---

## ğŸ¯ Key Design Decisions

### 1. Worktree Naming

**Recommendation:**
```python
worktree_id = f"{session_id}_{file.stem}"
# Example: stomper-20251008-153000_auth
```

**Why:**
- âœ… Ties back to session (for debugging)
- âœ… File name included (clear purpose)
- âœ… Unique per file
- âœ… Easy to track

### 2. Test Validation Levels

**Add configuration:**
```python
class TestValidation(str, Enum):
    FULL = "full"      # Full suite after each file (safest, slowest)
    QUICK = "quick"    # Affected tests only (faster, less safe)
    FINAL = "final"    # Once at session end (fastest, risky)
    NONE = "none"      # Skip tests (dangerous)

# Default
test_validation = TestValidation.FULL
```

**Implementation:**
```python
async def _run_test_suite(self, state: StomperState):
    validation_mode = state.get("test_validation", "full")
    
    if validation_mode == "full":
        # Run all tests
        test_errors = run_all_tests(worktree_path)
    elif validation_mode == "quick":
        # Run only tests for current file
        test_errors = run_file_tests(worktree_path, current_file)
    elif validation_mode == "final":
        # Skip for now, run once at end
        return state
    else:  # none
        logger.warning("âš ï¸ Test validation disabled!")
        return state
```

### 3. Parallel Safety

**Use asyncio.Lock:**
```python
class StomperWorkflow:
    def __init__(self, ...):
        # ...
        self._diff_application_lock = asyncio.Lock()
```

**Why:**
- âœ… Prevents race conditions when multiple workers apply diffs
- âœ… Ensures atomic applyâ†’commit operations
- âœ… Simple and robust
- âœ… Works with asyncio.gather() in parallel mode

---

## ğŸ”§ GitPython Usage Guide

### Common Operations

**1. Get Diff:**
```python
from git import Repo

repo = Repo(worktree_path)
diff = repo.git.diff("HEAD")  # Unstaged changes
diff = repo.git.diff("HEAD~1", "HEAD")  # Last commit
```

**2. Apply Patch:**
```python
main_repo = Repo(main_workspace)

# Option A: From file
main_repo.git.apply(patch_file_path)

# Option B: From string (need temp file)
with tempfile.NamedTemporaryFile(mode='w', suffix='.patch', delete=False) as f:
    f.write(diff_content)
    f.flush()
    main_repo.git.apply(f.name)
Path(f.name).unlink()
```

**3. Stage and Commit:**
```python
main_repo = Repo(main_workspace)

# Stage specific file
main_repo.index.add([str(file_path)])

# Commit
main_repo.index.commit(commit_message)
```

**4. Worktree Operations:**
```python
# Create (already in sandbox_manager)
repo.git.worktree("add", str(path), "-b", branch_name, "HEAD")

# Remove (already in sandbox_manager)
repo.git.worktree("remove", str(path), "--force")

# Delete branch (already in sandbox_manager)
repo.git.branch("-D", branch_name)
```

**All these methods already exist in your codebase!** âœ…

---

## ğŸ“Š Before & After Comparison

### State Management

**Before (Session-Level):**
```python
StomperState:
    session_id: "stomper-20251008-153000"
    sandbox_path: ".stomper/sandboxes/stomper-20251008-153000"  # ONE worktree
    files: [auth.py, models.py, utils.py]
    current_file_index: 0 â†’ 1 â†’ 2
    
# All files use SAME worktree
```

**After (File-Level):**
```python
StomperState:
    session_id: "stomper-20251008-153000"  # Session tracks overall run
    sandbox_path: None â†’ worktree_auth â†’ None â†’ worktree_models â†’ None
    current_worktree_id: None â†’ "..._auth" â†’ None â†’ "..._models" â†’ None
    files: [auth.py, models.py, utils.py]
    current_file_index: 0 â†’ 1 â†’ 2
    
# Each file gets NEW worktree, destroyed after use
```

### Worktree Lifecycle

**Before:**
```
t=0s:  Create worktree
t=10s: Fix auth.py in worktree
t=20s: Fix models.py in worktree  
t=30s: Fix utils.py in worktree
t=40s: Destroy worktree
```

**After:**
```
t=0s:  Create worktree_auth
t=10s: Fix auth.py, apply to main, destroy worktree_auth
t=11s: Create worktree_models
t=21s: Fix models.py, apply to main, destroy worktree_models
t=22s: Create worktree_utils
t=32s: Fix utils.py, apply to main, destroy worktree_utils
```

**Worktrees are ephemeral (10-30 seconds each)!** âœ…

---

## ğŸš€ Parallel Mode Preview (Phase 2)

With per-file worktrees, parallel becomes simple:

```python
async def run_parallel(self, config: dict[str, Any]) -> StomperState:
    """Run workflow in parallel mode."""
    
    # Collect errors in main workspace (single-threaded)
    files_with_errors = await self._collect_all_errors_async()
    
    # Worker pool
    max_workers = config.get("parallel_files", 4)
    semaphore = asyncio.Semaphore(max_workers)
    
    async def process_one_file(file, errors):
        async with semaphore:  # Max N concurrent worktrees
            worktree_id = f"{session_id}_{file.stem}"
            
            # Each worker: create â†’ fix â†’ verify â†’ test â†’ extract â†’ apply â†’ destroy
            worktree = create_worktree(worktree_id)
            
            try:
                fix_in_worktree(worktree, file, errors)
                verify(worktree, file)
                test(worktree)
                diff = extract_diff(worktree)
                
                # Serialize diff application (one at a time)
                async with self._diff_lock:
                    apply_to_main(diff)
                    commit_in_main(file)
                
                return {"success": True}
                
            finally:
                destroy_worktree(worktree)  # Always cleanup!
    
    # Process all files concurrently (up to max_workers)
    results = await asyncio.gather(*[
        process_one_file(file, errors)
        for file, errors in files_with_errors.items()
    ])
    
    return final_state
```

**Beautiful!** Each file completely independent! ğŸ‰

---

## âœ… Implementation Steps (Detailed)

### Step 1: Update Models (30 min)

**File:** `src/stomper/workflow/state.py`

```python
class StomperState(TypedDict, total=False):
    # ... existing fields ...
    
    # NEW: Per-file processing state
    current_prompt: str  # Prompt for current file
    current_diff: str | None  # Diff from current worktree
    current_worktree_id: str | None  # Current worktree identifier
```

### Step 2: Add Helper Methods (30 min)

**File:** `src/stomper/workflow/orchestrator.py`

```python
def _get_diff_lock(self) -> asyncio.Lock:
    """Get lock for diff application (parallel safety)."""
    if not hasattr(self, '_diff_application_lock'):
        import asyncio
        self._diff_application_lock = asyncio.Lock()
    return self._diff_application_lock

def _check_files_to_process(self, state: StomperState) -> str:
    """Check if there are files to process."""
    if not state.get("files") or len(state["files"]) == 0:
        return "no_files"
    return "has_files"

def _should_continue_after_error(self, state: StomperState) -> str:
    """Decide whether to continue after error."""
    # Could make this configurable
    continue_on_error = state.get("continue_on_error", True)
    
    if continue_on_error and state["current_file_index"] + 1 < len(state["files"]):
        return "continue"
    return "abort"
```

### Step 3: Implement New Nodes (3-4 hours)

**Add all 7 new nodes as detailed in Changes 3-9 above**

### Step 4: Refactor Existing Nodes (1 hour)

**Update nodes to work with new flow**

### Step 5: Rebuild Graph Structure (1 hour)

**Complete new edge structure**

### Step 6: Update Tests (1-2 hours)

**Modify tests to verify new behavior**

### Step 7: Verify & Document (1 hour)

**Run full test suite, update documentation**

---

## ğŸ¯ Success Criteria

### After Refactoring

- [ ] Each file gets its own worktree
- [ ] Worktree created right before fixing file
- [ ] Worktree destroyed immediately after applying diff
- [ ] Diffs applied to main workspace using GitPython
- [ ] Commits made in main workspace using GitPython
- [ ] No subprocess calls for git operations
- [ ] Lock prevents concurrent diff application
- [ ] All 273+ tests still pass
- [ ] New lifecycle test passes
- [ ] Documentation updated with new flow

---

## ğŸ“š Configuration Updates

### Add to WorkflowConfig

```python
class WorkflowConfig(BaseModel):
    use_sandbox: bool = True
    run_tests: bool = True
    max_retries: int = 3
    processing_strategy: str = "batch_errors"
    agent_name: str = "cursor-cli"
    
    # NEW fields
    test_validation: Literal["full", "quick", "final", "none"] = "full"
    files_per_worktree: int = 1  # MVP: 1, Future: configurable
    continue_on_error: bool = True  # Continue processing other files on error
```

---

## ğŸ‰ Benefits of This Architecture

### Immediate Benefits (MVP)

1. âœ… **Cleaner separation** - Each file isolated
2. âœ… **Faster cleanup** - Worktree lives ~30 seconds
3. âœ… **Smaller diffs** - One file at a time
4. âœ… **Better error handling** - Know which file failed
5. âœ… **Atomic operations** - File fix is all-or-nothing

### Future Benefits (Phase 2)

1. âœ… **Easy parallelization** - Just run workers concurrently
2. âœ… **Scalable** - Can run 10+ files simultaneously
3. âœ… **Independent workers** - One failure doesn't affect others
4. âœ… **Resource efficient** - Create worktrees on-demand
5. âœ… **Clean git history** - Main workspace commits only

---

## ğŸ“– Migration Notes

### Backwards Compatibility

**The refactoring maintains the same external API:**
```python
# Still works the same way
workflow = StomperWorkflow(project_root=path, use_sandbox=True, run_tests=True)
workflow.register_agent("cursor-cli", agent)
final_state = await workflow.run(config)
```

**Internal changes are transparent to users!** âœ…

### Breaking Changes

**None!** The refactoring is internal to the workflow orchestrator.

**Tests might need updates** to verify new lifecycle, but that's expected.

---

## ğŸš€ Next Steps

### To Implement This Plan

1. **Create feature branch**
   ```bash
   git checkout -b refactor/per-file-worktrees
   ```

2. **Follow implementation checklist** (phases 1-7)

3. **Run tests continuously**
   ```bash
   pytest tests/e2e/test_workflow_integration.py -k asyncio -v
   ```

4. **Update documentation** when complete

5. **Merge when all tests pass**

---

## ğŸ“ Estimated Timeline

| Phase | Task | Time | Cumulative |
|-------|------|------|------------|
| 1 | State & helpers | 1-2h | 2h |
| 2 | New nodes | 2-3h | 5h |
| 3 | Update existing | 1h | 6h |
| 4 | Rebuild graph | 1h | 7h |
| 5 | Update tests | 1-2h | 9h |
| 6 | Verify & doc | 1h | 10h |

**Total: 6-10 hours**

---

## ğŸŠ Conclusion

This refactoring will transform the workflow from:
- âŒ Session-level worktree (one for all files)
- âœ… File-level worktrees (one per file, ephemeral)

**Making it:**
- âœ… Safer (true file isolation)
- âœ… Faster (immediate cleanup)
- âœ… Parallel-ready (independent workers)
- âœ… Better architecture (granular nodes)
- âœ… Using GitPython (no subprocess)

**This matches your actual design perfectly!** ğŸŒŸ

---

**Ready to implement when you give the go-ahead!** ğŸš€

